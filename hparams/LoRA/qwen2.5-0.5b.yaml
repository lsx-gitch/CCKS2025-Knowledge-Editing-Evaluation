alg_name: "LoRA"
model_name: "c:/Users/86135/EasyEdit/hugging_cache/Qwen2.5-0.5B-Instruct"
device: "cpu"

# LoRA settings
lora_type: "lora"  # QLoRA typically uses standard LoRA, not AdaLoRA
rank: 8
lora_alpha: 32
layers: []
kl_factor: 0
norm_constraint: false
lora_dropout: 0.1
target_modules: ["q_proj", "v_proj"]

# Training settings
num_steps: 60
batch_size: 1
max_length: 256
lr: 5e-5
weight_decay: 0.0

# Additional settings
model_parallel: False